% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation_EvaluatorAccuracyROC.R
\name{EvaluatorAccuracy}
\alias{EvaluatorAccuracy}
\title{Compute Accuracy of a Classifier}
\usage{
EvaluatorAccuracy(.prediction, .dataset, .target, .threshold = 0.5)
}
\arguments{
\item{.prediction}{A \code{data.frame} containing the predictions and the true values as columns or a
numeric vector containing only the predictions. The true values have to be encoded by 0/1
or by TRUE/FALSE. The predicted values have to be numeric and be in a range of 0 to 1.}

\item{.dataset}{An optional \code{Dataset} or \code{data.frame} object, that has to be provided if \code{.prediction} is
a numeric vector.}

\item{.target}{A character vector of length one, being the name of the target variable contained as column
in the .dataset}

\item{.threshold}{An optional argument for setting the threshold at which a prediction gets assigned to a class.}
}
\description{
Evaluate the performance of a \code{ModelClassification} object on a binary classification problem
using the Accuracy. The Accuracy is computed as the number of correctly classified observations divided by
the total number of observations.
}
\examples{
x <- data.frame(var1 = c(1, 2, 3, 4, 5, 6, 7), target = c(1, 1, 1, 1, 0, 1, 0))
predictions <- c(1)
EvaluatorAccuracy(predictions, x, "target")
predictions <- data.frame(prediction = c(0.8, 0.2, 0.6, 0.8, 0.8), truth = c(1, 0, 1, 1, 1))
EvaluatorAccuracy(predictions)
EvaluatorAccuracy(.prediction = predictions, .threshold = 0.7)
}
\seealso{
\code{\link[=EvaluatorAUC]{EvaluatorAUC()}} for evaluating the AUC of a classifier, \code{\link[=EvaluatorMAE]{EvaluatorMAE()}} for computing the mean
absolute error, \code{\link[=EvaluatorMSE]{EvaluatorMSE()}} for the mean-squared error (corresponding to the Brier-Score in binary
classification).
}
